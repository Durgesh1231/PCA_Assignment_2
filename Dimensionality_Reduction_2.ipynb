{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9B_EK4Me9ix"
      },
      "outputs": [],
      "source": [
        "# Q1. What is a projection and how is it used in PCA?\n",
        "# In PCA, a projection is the process of mapping the original data points onto a lower-dimensional space.\n",
        "# The data is projected onto the principal components (the eigenvectors of the covariance matrix),\n",
        "# where the first component captures the most variance, and the last component captures the least variance.\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Apply PCA to reduce the data to 2 dimensions\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "print(\"Original shape of data:\", X.shape)\n",
        "print(\"Shape after PCA projection:\", X_pca.shape)\n",
        "\n",
        "# Q2. How does the optimization problem in PCA work, and what is it trying to achieve?\n",
        "# PCA seeks to solve the optimization problem where it tries to maximize the variance (spread of data)\n",
        "# in the projection space by finding the principal components (the eigenvectors of the covariance matrix).\n",
        "# The first principal component maximizes the variance, and each subsequent component is orthogonal and captures the next highest variance.\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Applying PCA to reduce to 2 components\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Q3. What is the relationship between covariance matrices and PCA?\n",
        "# In PCA, the covariance matrix represents how the features vary with respect to each other.\n",
        "# The eigenvectors of the covariance matrix give the directions of maximum variance, and the eigenvalues give the magnitude of the variance in those directions.\n",
        "\n",
        "# Calculating the covariance matrix\n",
        "cov_matrix = np.cov(X, rowvar=False)\n",
        "print(\"Covariance Matrix:\\n\", cov_matrix)\n",
        "\n",
        "# Q4. How does the choice of number of principal components impact the performance of PCA?\n",
        "# The number of components determines how much variance is retained in the reduced dimensionality.\n",
        "# If too few components are chosen, important information may be lost. If too many are chosen,\n",
        "# the data may not be sufficiently reduced, leading to overfitting.\n",
        "\n",
        "# Show explained variance ratio\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "\n",
        "print(\"Explained Variance Ratio per component:\", pca.explained_variance_ratio_)\n",
        "\n",
        "# Q5. How can PCA be used in feature selection, and what are the benefits of using it for this purpose?\n",
        "# PCA can be used for feature selection by selecting the top components that explain most of the variance in the data.\n",
        "# By reducing the number of features, we reduce dimensionality, computational cost, and potentially overfitting.\n",
        "\n",
        "# Q6. What are some common applications of PCA in data science and machine learning?\n",
        "# PCA is used in dimensionality reduction, noise reduction, data visualization, and in improving the performance of machine learning models by reducing overfitting.\n",
        "\n",
        "# Example of using PCA for data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Using the first two principal components for visualization\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y)\n",
        "plt.title('PCA: Iris dataset projected to 2D')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.show()\n",
        "\n",
        "# Q7. What is the relationship between spread and variance in PCA?\n",
        "# In PCA, variance is a measure of the spread of the data. The more spread the data has along a particular direction,\n",
        "# the higher the variance in that direction, and that direction becomes a principal component.\n",
        "\n",
        "# Q8. How does PCA use the spread and variance of the data to identify principal components?\n",
        "# PCA identifies principal components by finding the directions of maximum spread (variance) in the data.\n",
        "# The first component captures the direction of the largest variance, the second captures the next largest, and so on.\n",
        "\n",
        "# Q9. How does PCA handle data with high variance in some dimensions but low variance in others?\n",
        "# PCA handles this by giving more importance to dimensions with high variance (through the corresponding eigenvalues),\n",
        "# while dimensions with low variance contribute less to the principal components.\n",
        "\n",
        "# Visualize explained variance ratio to observe how PCA handles the variance\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.title('Cumulative Explained Variance by Principal Components')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.show()\n"
      ]
    }
  ]
}